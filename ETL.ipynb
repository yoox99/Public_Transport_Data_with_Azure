{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "248101b7-e86d-43a1-a24c-cfc257388bc8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Data Lake Infrastructure Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28de34de-09de-4b15-94cc-ed33347edbbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Configure Spark to access Azure Data Lake Storage Gen2 using the account key\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.yasseraithninistorage.dfs.core.windows.net\", \n",
    "    \"vWNXPK8c521JLzUwOSCdhbSMlOmRQvJdQOW2IpC47oA55hDibDLFzxKVekgetrjB1FwEErdh+Qrx+AStIb1H7w==\"\n",
    ")              \n",
    "\n",
    "# List the contents of the 'raw' folder in Azure Data Lake Storage Gen2\n",
    "dbutils.fs.ls(\"abfss://publictransportdata@yasseraithninistorage.dfs.core.windows.net/public_transport_data/raw/\")\n",
    "\n",
    "#Set data lake file location\n",
    "file_location = \"abfss://publictransportdata@yasseraithninistorage.dfs.core.windows.net/public_transport_data/raw/public-transport-data.csv\"\n",
    "#read in the data to dataframe df\n",
    "df = spark.read.format(\"csv\").option(\"inferSchema\", \"True\").option(\"header\",\n",
    "\"True\").option(\"delimeter\",\",\").load(file_location) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25258183-43ba-4993-9066-91fdbebcc6c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display the dataframe\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9597a22-a8cd-4dc1-81ed-ecd88171d8e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "642a234e-3522-441b-a29c-4ee6e4b8d47e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#ETL processes with Azure Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84e55236-8d3b-4b82-8c76-2d3d732bae59",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Time calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db31f533-9a18-4eb8-bfa1-243db38c0208",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, date_format\n",
    "\n",
    "# Format 'DepartureTime' to display only the time\n",
    "df = df.withColumn(\"DepartureTime\", date_format(\"DepartureTime\", \"HH:mm\"))\n",
    "\n",
    "# Format 'DepartureTime' to display only the time\n",
    "df = df.withColumn(\"DepartureTime\", date_format(\"DepartureTime\", \"HH:mm\"))\n",
    "\n",
    "# Assuming you have the columns ArrivalTime and DepartureTime\n",
    "df = df.withColumn(\"ArrivalHour\", expr(\"CAST(split(ArrivalTime, ':')[0] AS INT)\"))\n",
    "df = df.withColumn(\"ArrivalMinute\", expr(\"CAST(split(ArrivalTime, ':')[1] AS INT)\"))\n",
    "df = df.withColumn(\"DepartureHour\", expr(\"CAST(split(DepartureTime, ':')[0] AS INT)\"))\n",
    "df = df.withColumn(\"DepartureMinute\", expr(\"CAST(split(DepartureTime, ':')[1] AS INT)\"))\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"Duration(min)\",\n",
    "    (col(\"ArrivalHour\") * 60 + col(\"ArrivalMinute\")) - (col(\"DepartureHour\") * 60 + col(\"DepartureMinute\"))\n",
    ")\n",
    "\n",
    "columns_to_drop = ['ArrivalHour', 'ArrivalMinute', 'DepartureHour', 'DepartureMinute']\n",
    "df = df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adf96df2-d03c-46c3-888d-1fe8dad9afa8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display the dataframe\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fbbf2f2-d7f0-4f9a-985e-e1f84317914e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Column of ArrivalTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29dfadc6-5bc4-4131-8771-1e0785cd93ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, expr, to_timestamp, date_format\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "df = df.withColumn(\n",
    "    \"ArrivalTime\",\n",
    "    when(\n",
    "        expr(\"substring(ArrivalTime, 1, 2) > 23\"),\n",
    "        expr(\"concat('00', substring(ArrivalTime, 3, 5))\")\n",
    "    ).otherwise(col(\"ArrivalTime\"))\n",
    ")\n",
    "\n",
    "# Convert the 'ArrivalTime' column to a time type\n",
    "df = df.withColumn(\"ArrivalTime\", to_timestamp(\"ArrivalTime\", \"HH:mm\"))\n",
    "\n",
    "# Format 'ArrivalTime' to display only the time\n",
    "df = df.withColumn(\"ArrivalTime\", date_format(\"ArrivalTime\", \"HH:mm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f3d2596-ce43-49ff-8d5f-6609779a768b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display the dataframe\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dc0fe38-550e-4509-bd0b-9d8f29f17ff1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Date Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68d3a287-b1cf-414f-b65a-c64d4161b0fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extract the year, month, day, and day of the week from the date\n",
    "\n",
    "from pyspark.sql.functions import to_date, year, month, dayofmonth, dayofweek\n",
    "df = df.withColumn(\"Date\", to_date(\"Date\", \"yyyy-MM-dd\"))\n",
    "df = df.withColumn(\"Year\", year(\"Date\"))\n",
    "df = df.withColumn(\"Month\", month(\"Date\"))\n",
    "df = df.withColumn(\"DayOfMonth\", dayofmonth(\"Date\"))\n",
    "df = df.withColumn(\"DayOfWeek\", dayofweek(\"Date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f5a023-9dd9-4dea-8f6f-f20a319245eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7b7f722-2274-4d46-819e-919004e5ca82",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Delay Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a96b8950-a936-4c5b-9205-f4358fc093cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Assuming you have a column named 'Delay' which represents the delay in minutes\n",
    "\n",
    "# Create a new column 'DelayCategory' based on the delay duration\n",
    "df = df.withColumn(\n",
    "    \"DelayCategory\",\n",
    "    when(col(\"Delay\") <= 0, \"Pas de Retard\")\n",
    "    .when((col(\"Delay\") > 0) & (col(\"Delay\") <= 10), \"Retard Court\")\n",
    "    .when((col(\"Delay\") > 10) & (col(\"Delay\") <= 20), \"Retard Moyen\")\n",
    "    .when(col(\"Delay\") > 20, \"Long Retard\")\n",
    "    .otherwise(\"Unknown\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c24206-b7e5-4939-9a15-47f7b2a9d5dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae287d59-7272-4870-a3d4-51c2882fdabb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Passenger Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7737d59e-8716-459d-8542-3a7cd5140250",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit, mean\n",
    "\n",
    "# Calculate the mean of the PassengerCount column\n",
    "mean_passenger_count = df.select(mean(df[\"Passengers\"])).collect()[0][0]\n",
    "\n",
    "# Use the mean as the threshold to identify peak hours\n",
    "df = df.withColumn(\"PeakHour\", when(df[\"Passengers\"] > mean_passenger_count, \"Peak\").otherwise(\"Off-Peak\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9940348f-6094-4b50-b5c8-96140b099da4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a77e1de5-313d-4bf1-ade9-e7a905cf50d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Route Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1da683b-5c06-43e5-a42f-c70e4b876ac3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, sum\n",
    "\n",
    "# Group by the 'Route' column and calculate mean delay, mean passengers, and total trips\n",
    "route_analysis = df.groupBy(\"Route\").agg(\n",
    "    avg(\"Delay\").cast(\"int\").alias(\"MeanDelay\"),\n",
    "    avg(\"Passengers\").cast(\"int\").alias(\"MeanPassengers\"),\n",
    "    sum(lit(1)).cast(\"int\").alias(\"TotalTrips\")\n",
    ")\n",
    "\n",
    "# Show the route analysis\n",
    "route_analysis.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ETL",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
